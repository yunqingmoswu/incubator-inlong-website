"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[8642],{3905:function(t,e,i){i.d(e,{Zo:function(){return c},kt:function(){return m}});var n=i(67294);function o(t,e,i){return e in t?Object.defineProperty(t,e,{value:i,enumerable:!0,configurable:!0,writable:!0}):t[e]=i,t}function l(t,e){var i=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),i.push.apply(i,n)}return i}function r(t){for(var e=1;e<arguments.length;e++){var i=null!=arguments[e]?arguments[e]:{};e%2?l(Object(i),!0).forEach((function(e){o(t,e,i[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(i)):l(Object(i)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(i,e))}))}return t}function a(t,e){if(null==t)return{};var i,n,o=function(t,e){if(null==t)return{};var i,n,o={},l=Object.keys(t);for(n=0;n<l.length;n++)i=l[n],e.indexOf(i)>=0||(o[i]=t[i]);return o}(t,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(t);for(n=0;n<l.length;n++)i=l[n],e.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(t,i)&&(o[i]=t[i])}return o}var s=n.createContext({}),d=function(t){var e=n.useContext(s),i=e;return t&&(i="function"==typeof t?t(e):r(r({},e),t)),i},c=function(t){var e=d(t.components);return n.createElement(s.Provider,{value:e},t.children)},u={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},p=n.forwardRef((function(t,e){var i=t.components,o=t.mdxType,l=t.originalType,s=t.parentName,c=a(t,["components","mdxType","originalType","parentName"]),p=d(i),m=o,h=p["".concat(s,".").concat(m)]||p[m]||u[m]||l;return i?n.createElement(h,r(r({ref:e},c),{},{components:i})):n.createElement(h,r({ref:e},c))}));function m(t,e){var i=arguments,o=e&&e.mdxType;if("string"==typeof t||o){var l=i.length,r=new Array(l);r[0]=p;var a={};for(var s in e)hasOwnProperty.call(e,s)&&(a[s]=e[s]);a.originalType=t,a.mdxType="string"==typeof t?t:o,r[1]=a;for(var d=2;d<l;d++)r[d]=i[d];return n.createElement.apply(null,r)}return n.createElement.apply(null,i)}p.displayName="MDXCreateElement"},97853:function(t,e,i){i.r(e),i.d(e,{assets:function(){return c},contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return a},metadata:function(){return d},toc:function(){return u}});var n=i(87462),o=i(63366),l=(i(67294),i(3905)),r=["components"],a={title:"HDFS",sidebar_position:11},s=void 0,d={unversionedId:"data_node/load_node/hdfs",id:"data_node/load_node/hdfs",title:"HDFS",description:"HDFS Load Node",source:"@site/docs/data_node/load_node/hdfs.md",sourceDirName:"data_node/load_node",slug:"/data_node/load_node/hdfs",permalink:"/docs/next/data_node/load_node/hdfs",draft:!1,editUrl:"https://github.com/apache/incubator-inlong-website/edit/master/docs/data_node/load_node/hdfs.md",tags:[],version:"current",sidebarPosition:11,frontMatter:{title:"HDFS",sidebar_position:11},sidebar:"tutorialSidebar",previous:{title:"HBase",permalink:"/docs/next/data_node/load_node/hbase"},next:{title:"MySQL",permalink:"/docs/next/data_node/load_node/mysql"}},c={},u=[{value:"HDFS Load Node",id:"hdfs-load-node",level:2},{value:"How to create a HDFS Load Node",id:"how-to-create-a-hdfs-load-node",level:2},{value:"Usage for SQL API",id:"usage-for-sql-api",level:3},{value:"File Formats",id:"file-formats",level:4},{value:"Rolling Policy",id:"rolling-policy",level:4},{value:"File Compaction",id:"file-compaction",level:4},{value:"Partition Commit",id:"partition-commit",level:4},{value:"Partition Commit Policy",id:"partition-commit-policy",level:4}],p={toc:u};function m(t){var e=t.components,i=(0,o.Z)(t,r);return(0,l.kt)("wrapper",(0,n.Z)({},p,i,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("h2",{id:"hdfs-load-node"},"HDFS Load Node"),(0,l.kt)("p",null,"HDFS uses the general capabilities of flink's fileSystem to support single files and partitioned files.\nThe file system connector itself is included in Flink and does not require an additional dependency.\nThe corresponding jar can be found in the Flink distribution inside the /lib directory.\nA corresponding format needs to be specified for reading and writing rows from and to a file system."),(0,l.kt)("h2",{id:"how-to-create-a-hdfs-load-node"},"How to create a HDFS Load Node"),(0,l.kt)("h3",{id:"usage-for-sql-api"},"Usage for SQL API"),(0,l.kt)("p",null,"The example below shows how to create a HDFS Load Node with ",(0,l.kt)("inlineCode",{parentName:"p"},"Flink SQL Cli")," :"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE hdfs_load_node (\n  id STRING,\n  name STRING,\n  uv BIGINT,\n  pv BIGINT,\n  dt STRING,\n `hour` STRING\n  ) PARTITIONED BY (dt, `hour`) WITH (\n    'connector'='filesystem',\n    'path'='...',\n    'format'='orc',\n    'sink.partition-commit.delay'='1 h',\n    'sink.partition-commit.policy.kind'='success-file'\n  );\n")),(0,l.kt)("h4",{id:"file-formats"},"File Formats"),(0,l.kt)("ul",null,(0,l.kt)("li",null,"CSV(Uncompressed)"),(0,l.kt)("li",null,"JSON(JSON format for file system connector is not a typical JSON file but uncompressed newline delimited JSON.)"),(0,l.kt)("li",null,"Avro(Support compression by configuring avro.codec.)"),(0,l.kt)("li",null,"Parquet(Compatible with Hive.)"),(0,l.kt)("li",null,"Orc(Compatible with Hive.)"),(0,l.kt)("li",null,"Debezium-JSON"),(0,l.kt)("li",null,"Canal-JSON"),(0,l.kt)("li",null,"Raw")),(0,l.kt)("h4",{id:"rolling-policy"},"Rolling Policy"),(0,l.kt)("p",null,"Data within the partition directories are split into part files.\nEach partition will contain at least one part file for each subtask of the sink that has received data for that partition.\nThe in-progress part file will be closed and additional part file will be created according to the configurable rolling policy.\nThe policy rolls part files based on size, a timeout that specifies the maximum duration for which a file can be open."),(0,l.kt)("table",{class:"table table-bordered"},(0,l.kt)("thead",null,(0,l.kt)("tr",null,(0,l.kt)("th",{class:"text-left",style:{width:"25%"}},"Option"),(0,l.kt)("th",{class:"text-center",style:{width:"7%"}},"Default"),(0,l.kt)("th",{class:"text-center",style:{width:"10%"}},"Type"),(0,l.kt)("th",{class:"text-center",style:{width:"50%"}},"Description"))),(0,l.kt)("tbody",null,(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"sink.rolling-policy.file-size")),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"128MB"),(0,l.kt)("td",null,"MemorySize"),(0,l.kt)("td",null,"The maximum part file size before rolling.")),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"sink.rolling-policy.rollover-interval")),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"30 min"),(0,l.kt)("td",null,"String"),(0,l.kt)("td",null,"The maximum time duration a part file can stay open before rolling (by default 30 min to avoid to many small files). The frequency at which this is checked is controlled by the 'sink.rolling-policy.check-interval' option.")),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"sink.rolling-policy.check-interval")),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"1 min"),(0,l.kt)("td",null,"String"),(0,l.kt)("td",null,"The interval for checking time based rolling policies. This controls the frequency to check whether a part file should rollover based on 'sink.rolling-policy.rollover-interval'.")))),(0,l.kt)("h4",{id:"file-compaction"},"File Compaction"),(0,l.kt)("p",null,"The file sink supports file compactions, which allows applications to have smaller checkpoint intervals without generating a large number of files."),(0,l.kt)("table",{class:"table table-bordered"},(0,l.kt)("thead",null,(0,l.kt)("tr",null,(0,l.kt)("th",{class:"text-left",style:{width:"25%"}},"Option"),(0,l.kt)("th",{class:"text-center",style:{width:"7%"}},"Default"),(0,l.kt)("th",{class:"text-center",style:{width:"10%"}},"Type"),(0,l.kt)("th",{class:"text-center",style:{width:"50%"}},"Description"))),(0,l.kt)("tbody",null,(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"auto-compaction")),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"false"),(0,l.kt)("td",null,"Boolean"),(0,l.kt)("td",null,"Whether to enable automatic compaction in streaming sink or not. The data will be written to temporary files. After the checkpoint is completed, the temporary files generated by a checkpoint will be compacted. The temporary files are invisible before compaction.")),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"compaction.file-size")),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"(none)"),(0,l.kt)("td",null,"String"),(0,l.kt)("td",null,"The compaction target file size, the default value is the rolling file size.")))),(0,l.kt)("h4",{id:"partition-commit"},"Partition Commit"),(0,l.kt)("p",null,"After writing a partition, it is often necessary to notify downstream applications.\nFor example, add the partition to a Hive metastore or writing a _SUCCESS file in the directory.\nThe file system sink contains a partition commit feature that allows configuring custom policies.\nCommit actions are based on a combination of triggers and policies."),(0,l.kt)("table",{class:"table table-bordered"},(0,l.kt)("thead",null,(0,l.kt)("tr",null,(0,l.kt)("th",{class:"text-left",style:{width:"25%"}},"Option"),(0,l.kt)("th",{class:"text-center",style:{width:"7%"}},"Default"),(0,l.kt)("th",{class:"text-center",style:{width:"10%"}},"Type"),(0,l.kt)("th",{class:"text-center",style:{width:"50%"}},"Description"))),(0,l.kt)("tbody",null,(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"sink.partition-commit.trigger")),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"process-time"),(0,l.kt)("td",null,"String"),(0,l.kt)("td",null,"Trigger type for partition commit: 'process-time': based on the time of the machine, it neither requires partition time extraction nor watermark generation. Commit partition once the 'current system time' passes 'partition creation system time' plus 'delay'. 'partition-time': based on the time that extracted from partition values, it requires watermark generation. Commit partition once the 'watermark' passes 'time extracted from partition values' plus 'delay'.")),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"sink.partition-commit.delay")),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"0 s"),(0,l.kt)("td",null,"Duration"),(0,l.kt)("td",null,"The partition will not commit until the delay time. If it is a daily partition, should be '1 d', if it is a hourly partition, should be '1 h'.")),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"sink.partition-commit.watermark-time-zone")),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"UTC"),(0,l.kt)("td",null,"String"),(0,l.kt)("td",null,"The time zone to parse the long watermark value to TIMESTAMP value, the parsed watermark timestamp is used to compare with partition time to decide the partition should commit or not. This option is only take effect when `sink.partition-commit.trigger` is set to 'partition-time'. If this option is not configured correctly, e.g. source rowtime is defined on TIMESTAMP_LTZ column, but this config is not configured, then users may see the partition committed after a few hours. The default value is 'UTC', which means the watermark is defined on TIMESTAMP column or not defined. If the watermark is defined on TIMESTAMP_LTZ column, the time zone of watermark is the session time zone. The option value is either a full name such as 'America/Los_Angeles', or a custom timezone id such as 'GMT-08:00'.")))),(0,l.kt)("h4",{id:"partition-commit-policy"},"Partition Commit Policy"),(0,l.kt)("p",null,"The partition strategy defines the specific operation of partition submission."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"metastore\uff1aThis strategy is only supported when hive."),(0,l.kt)("li",{parentName:"ul"},"success: The '_SUCCESS' file will be generated after the part file is generated.")),(0,l.kt)("table",{class:"table table-bordered"},(0,l.kt)("thead",null,(0,l.kt)("tr",null,(0,l.kt)("th",{class:"text-left",style:{width:"25%"}},"Option"),(0,l.kt)("th",{class:"text-left",style:{width:"8%"}},"Required"),(0,l.kt)("th",{class:"text-center",style:{width:"7%"}},"Default"),(0,l.kt)("th",{class:"text-center",style:{width:"10%"}},"Type"),(0,l.kt)("th",{class:"text-center",style:{width:"50%"}},"Description"))),(0,l.kt)("tbody",null,(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"sink.partition-commit.policy.kind")),(0,l.kt)("td",null,"optional"),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"(none)"),(0,l.kt)("td",null,"String"),(0,l.kt)("td",null,"Policy to commit a partition is to notify the downstream application that the partition has finished writing, the partition is ready to be read. metastore: add partition to metastore. Only hive table supports metastore policy, file system manages partitions through directory structure. success-file: add '_success' file to directory. Both can be configured at the same time: 'metastore,success-file'. custom: use policy class to create a commit policy. Support to configure multiple policies: 'metastore,success-file'.")),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"sink.partition-commit.policy.class")),(0,l.kt)("td",null,"optional"),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"(none)"),(0,l.kt)("td",null,"String"),(0,l.kt)("td",null,"The partition commit policy class for implement PartitionCommitPolicy interface. Only work in custom commit policy.")),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("h5",null,"sink.partition-commit.success-file.name")),(0,l.kt)("td",null,"optional"),(0,l.kt)("td",{style:{wordWrap:"break-word"}},"_SUCCESS"),(0,l.kt)("td",null,"String"),(0,l.kt)("td",null,"The file name for success-file partition commit policy, default is '_SUCCESS'.")))))}m.isMDXComponent=!0}}]);